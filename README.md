# CS231n_Spring_2019

Solutions to [Stanford CS231n Spring 2019 Course](http://cs231n.stanford.edu/2019/) assignments.










If there are any mistakes, or you have questions about solutions, please contact me via github or e-mail.  My e-mail is [firstname][lastname]@gmail.com (my name is Nathan Bendich).  I apologize if I do not immediately get to you.  I would love to give an in-depth walkthrough of every detail to every person who contacts me, but unfortunately time probably will not permit that.


## A few resources I found helpful:
  1. Visualizations of Convolution's backward pass:  https://github.com/vdumoulin/conv_arithmetic
  2. Backpropagation notes from Karpathy: http://cs231n.github.io/optimization-2/
  3. GAN Training:  https://github.com/soumith/ganhacks
  4. Deep Learning Reading List: http://deeplearning.net/reading-list/ Â 
  5. Taking partial derivatives (called "Gradients" but more technically should be called "Jacobians") by hand: http://cs231n.stanford.edu/vecDerivs.pdf
  6. Softmax & 2-layer NN derivation by Karpathy:  http://cs231n.github.io/neural-networks-case-study/

### Things I wish I'd known when starting the lectures:
As far as I can tell, Xavier initialization is not so necessary any more.  Batch/Group/Instance/Layer normalization fixes the old problems Hinton et al. had with properly initializing neural networks.  Dropout still seems very relevant.  Nowadays skip connections in ResNet, ResNeXT, etc. make the early lecture content about regularization less relevant/totally outdated

























































